{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T14:13:13.155274Z",
     "start_time": "2025-01-20T14:13:03.906390Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer,TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T14:13:13.164061Z",
     "start_time": "2025-01-20T14:13:13.156280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fine_tune_lithuanian_model():\n",
    "    \"\"\"\n",
    "    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ª–∏—Ç–æ–≤—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "    \"\"\"\n",
    "    model_name = \"xlm-roberta-base\"\n",
    "    dataset_name = \"SkyWater21/lt_go_emotions\"\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7, ignore_mismatched_sizes=True)  # 28 –∫–∞—Ç–µ–≥–æ—Ä–∏–π —ç–º–æ—Ü–∏–π\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    \n",
    "    def preprocess_labels(example):\n",
    "        # Find the index of the first '1' in the labels list\n",
    "        label_index = example[\"labels\"].index(1) if 1 in example[\"labels\"] else 0 \n",
    "        example[\"labels\"] = label_index  # Use the index as the single label\n",
    "        return example\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"lt_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    dataset = dataset.map(preprocess_labels)\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",          # –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        evaluation_strategy=\"epoch\",    # —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ü–µ–Ω–∫–∏\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "    trainer.train()\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "    trainer.save_model(r\"C:\\Users\\Viktor\\Documents\\TMS\\NLP\\NLP_psyhology_support\\models\\lt_emotion_model_v1_1\")\n",
    "    print(\"–õ–∏—Ç–æ–≤—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\")"
   ],
   "id": "229de954da5763a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:31.273742Z",
     "start_time": "2025-01-20T14:13:13.164452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # –î–æ–æ–±—É—á–µ–Ω–∏–µ –ª–∏—Ç–æ–≤—Å–∫–æ–π –º–æ–¥–µ–ª–∏\n",
    "    fine_tune_lithuanian_model()"
   ],
   "id": "dcf01e7af6e6daf2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "717226abdf6546ca876e198f0e6c5605"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Viktor\\Documents\\TMS\\NLP\\NLP_psyhology_support\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Viktor\\AppData\\Local\\Temp\\ipykernel_16816\\217913038.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 22:03:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.020916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.020386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.019649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–∏—Ç–æ–≤—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:47:51.502638Z",
     "start_time": "2025-01-21T12:47:49.789089Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a9d4b0fd19c73d6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39msave_model(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mViktor\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDocuments\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mTMS\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mNLP\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mNLP_psyhology_support\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mlt_emotion_model_v1_1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m–õ–∏—Ç–æ–≤—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:37:21.420215Z",
     "start_time": "2025-01-21T12:48:49.255304Z"
    }
   },
   "cell_type": "code",
   "source": "fine_tune_lithuanian_model()",
   "id": "368b18daba2b89b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Viktor\\Documents\\TMS\\NLP\\NLP_psyhology_support\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Viktor\\AppData\\Local\\Temp\\ipykernel_16816\\217913038.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8142' max='8142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8142/8142 22:48:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.020780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.020255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.018390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–õ–∏—Ç–æ–≤—Å–∫–∞—è –º–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T12:17:31.326895Z",
     "start_time": "2025-01-21T12:17:31.309862Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1402e2117d35da93",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1ece9ddf66f17d25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
